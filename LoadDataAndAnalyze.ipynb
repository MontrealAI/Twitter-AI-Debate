{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LoadDataAndAnalyze.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9elFTeRBJ2-n","colab_type":"text"},"source":["# Initial Requirements"]},{"cell_type":"code","metadata":{"id":"k4RQTwreMWpU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"6b07d192-9a1c-45d2-fefe-40919a183c8e","executionInfo":{"status":"ok","timestamp":1578017464691,"user_tz":-210,"elapsed":19107,"user":{"displayName":"Peyman Ghasemi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCc9ANnnVEQ6qW-B2mMkrQfM-IdrpaQNO8I-C83=s64","userId":"11152285370379683023"}}},"source":["# Mount your google drive to the colab environment\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# copy data\n","!cp '/content/gdrive/My Drive/Colab Notebooks/Twitter_Analysis/OLD_tweets_with_hashtag_AIDebate.txt' before_debate.txt\n","!cp '/content/gdrive/My Drive/Colab Notebooks/Twitter_Analysis/tweets_with_hashtag_AIDebate1.txt' after_debate.txt\n","\n","\n","\n","\n","# change enviroment variable of keras's backend to theano\n","import os; os.environ['KERAS_BACKEND'] = 'theano'\n","\n","# get and install emotion predictor model\n","!git clone https://github.com/nikicc/twitter-emotion-recognition.git \n","!ls # list of files\n","\n","# go to the directory\n","%cd twitter-emotion-recognition/\n","\n","# install required versions of the libraries\n","!pip install -r requirements.txt\n","\n","\n","\n","\n","\n","# install library for language detection\n","!pip install langdetect\n","\n","# download NLTK packages\n","import nltk\n","!python -m nltk.downloader all"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","fatal: destination path 'twitter-emotion-recognition' already exists and is not an empty directory.\n","after_debate.txt   gdrive\tsample_data\n","before_debate.txt  newfile.txt\ttwitter-emotion-recognition\n","/content/twitter-emotion-recognition\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.25.3)\n","Requirement already satisfied: keras==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (1.1.0)\n","Requirement already satisfied: theano==0.8.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 1)) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 1)) (2.6.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 1)) (1.17.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras==1.1.0->-r requirements.txt (line 2)) (1.12.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==1.1.0->-r requirements.txt (line 2)) (3.13)\n","Requirement already satisfied: scipy>=0.11 in /usr/local/lib/python3.6/dist-packages (from theano==0.8.2->-r requirements.txt (line 3)) (1.3.3)\n","Requirement already satisfied: langdetect in /usr/local/lib/python3.6/dist-packages (1.0.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect) (1.12.0)\n","/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n","  warn(RuntimeWarning(msg))\n","[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Package abc is already up-to-date!\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Package alpino is already up-to-date!\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Package brown is already up-to-date!\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Package brown_tei is already up-to-date!\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Package cess_cat is already up-to-date!\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Package cess_esp is already up-to-date!\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Package chat80 is already up-to-date!\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package city_database is already up-to-date!\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Package cmudict is already up-to-date!\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package comparative_sentences is already up-to-\n","[nltk_data]    |       date!\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    |   Package comtrans is already up-to-date!\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Package conll2000 is already up-to-date!\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Package conll2002 is already up-to-date!\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    |   Package conll2007 is already up-to-date!\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Package crubadan is already up-to-date!\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package dependency_treebank is already up-to-date!\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Package dolch is already up-to-date!\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package europarl_raw is already up-to-date!\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Package floresta is already up-to-date!\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package framenet_v15 is already up-to-date!\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package framenet_v17 is already up-to-date!\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Package gazetteers is already up-to-date!\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Package genesis is already up-to-date!\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Package gutenberg is already up-to-date!\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Package ieer is already up-to-date!\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Package inaugural is already up-to-date!\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Package indian is already up-to-date!\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    |   Package jeita is already up-to-date!\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Package kimmo is already up-to-date!\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    |   Package knbc is already up-to-date!\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Package mac_morpho is already up-to-date!\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    |   Package machado is already up-to-date!\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    |   Package masc_tagged is already up-to-date!\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package moses_sample is already up-to-date!\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package movie_reviews is already up-to-date!\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Package names is already up-to-date!\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Package nps_chat is already up-to-date!\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Package omw is already up-to-date!\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Package paradigms is already up-to-date!\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Package pil is already up-to-date!\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Package pl196x is already up-to-date!\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Package ppattach is already up-to-date!\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package problem_reports is already up-to-date!\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    |   Package propbank is already up-to-date!\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Package ptb is already up-to-date!\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Package pros_cons is already up-to-date!\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Package qc is already up-to-date!\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    |   Package reuters is already up-to-date!\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Package rte is already up-to-date!\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    |   Package semcor is already up-to-date!\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Package senseval is already up-to-date!\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package sentiwordnet is already up-to-date!\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package sentence_polarity is already up-to-date!\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Package shakespeare is already up-to-date!\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package sinica_treebank is already up-to-date!\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Package smultron is already up-to-date!\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Package state_union is already up-to-date!\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Package stopwords is already up-to-date!\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package subjectivity is already up-to-date!\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Package swadesh is already up-to-date!\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Package switchboard is already up-to-date!\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Package timit is already up-to-date!\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Package toolbox is already up-to-date!\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Package treebank is already up-to-date!\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package twitter_samples is already up-to-date!\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Package udhr is already up-to-date!\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Package udhr2 is already up-to-date!\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package unicode_samples is already up-to-date!\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n","[nltk_data]    |       date!\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Package verbnet is already up-to-date!\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Package verbnet3 is already up-to-date!\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Package webtext is already up-to-date!\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Package wordnet is already up-to-date!\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Package wordnet_ic is already up-to-date!\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Package words is already up-to-date!\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Package ycoe is already up-to-date!\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Package rslp is already up-to-date!\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n","[nltk_data]    |       to-date!\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package universal_tagset is already up-to-date!\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Package punkt is already up-to-date!\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package book_grammars is already up-to-date!\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package sample_grammars is already up-to-date!\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package spanish_grammars is already up-to-date!\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package basque_grammars is already up-to-date!\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package large_grammars is already up-to-date!\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Package tagsets is already up-to-date!\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package snowball_data is already up-to-date!\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package word2vec_sample is already up-to-date!\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Package mte_teip5 is already up-to-date!\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n","[nltk_data]    |       to-date!\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n","[nltk_data]    |       up-to-date!\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package perluniprops is already up-to-date!\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package vader_lexicon is already up-to-date!\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Package porter_test is already up-to-date!\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Package wmt15_eval is already up-to-date!\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Jo_cDZA-MoBa","colab_type":"text"},"source":["# Emotion Analysis Part"]},{"cell_type":"code","metadata":{"id":"vRzOMNpvMlcw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"a6b97fb0-65d7-4a0e-d861-c35476f5eb12","executionInfo":{"status":"ok","timestamp":1578017464980,"user_tz":-210,"elapsed":19371,"user":{"displayName":"Peyman Ghasemi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCc9ANnnVEQ6qW-B2mMkrQfM-IdrpaQNO8I-C83=s64","userId":"11152285370379683023"}}},"source":["\n","\n","\n","# analyze each tweet\n","import datetime\n","from emotion_predictor import EmotionPredictor\n","\n","\n","\n","class TweetEmotion:\n","  \n","  # load emotion prediction model\n","  def __init__(self):\n","    self.model = EmotionPredictor(classification='ekman', setting='mc')\n","\n","\n","  # what emotion?\n","  def what_emotion(self, tweet):\n","    prediction = self.model.predict_classes([tweet])\n","    # print('The associated emotion with [' + tweet + '] seems to be:' + '\\n' + str(prediction))\n","    return prediction\n","\n","  # probability of emotion\n","  def probability_of_emotion(self, tweet):\n","    probability = self.model.predict_probabilities([tweet])\n","    # print('The associated probablity of emotion with [' + tweet + '] seems to be:' + '\\n' + str(probability))\n","    # return [probability['Anger'], probability['Disgust'], probability['Fear'],probability['Joy'], probability['Sadness'], probability['Surprise']]\n","    p = pd.Series({'Anger': float(probability['Anger']),'Disgust': float(probability['Disgust']), 'Fear': float(probability['Fear']), 'Joy': float(probability['Joy']), 'Sadness': float(probability['Sadness']),  'Surprise': float(probability['Surprise'])})\n","    return p\n","\n","\n","\n","# tweet_emotion = TweetEmotion()\n","\n","\n","\n","\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using Theano backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"72CC1L1PYk2I","colab_type":"text"},"source":["# Tweet Cleaning and Sentiment Analysis"]},{"cell_type":"code","metadata":{"id":"qvjxUX0iYsxM","colab_type":"code","colab":{}},"source":["from textblob import TextBlob\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize.treebank import TreebankWordDetokenizer\n","import langdetect\n","\n","def sentiment_and_freq(tweet):\n","\n","  # check whether tweet's language is English\n","  is_english = False\n","  lang = langdetect.detect(tweet)\n","  if(lang != 'en'):\n","    is_english = False\n","    # print(\"non-English\")\n","  else:\n","    # print(\"Yeah, English\")\n","    is_english = True\n","\n","\n","  # Clean the Tweet\n","\n","  # split into words\n","  tokens = word_tokenize(tweet)\n","  # convert to lower case\n","  tokens = [w.lower() for w in tokens]\n","  # remove punctuation from each word\n","  import string\n","  table = str.maketrans('', '', string.punctuation)\n","  stripped = [w.translate(table) for w in tokens]\n","  # remove remaining tokens that are not alphabetic\n","  words = [word for word in stripped if word.isalpha()]\n","  # filter out stop words\n","  from nltk.corpus import stopwords\n","  stop_words = set(stopwords.words('english'))\n","  words = [w for w in words if not w in stop_words]\n","\n","\n","\n","\n","  # detokenize to calculate sentiment polarity\n","  cleaned_sentence = TreebankWordDetokenizer().detokenize(words)\n","  polarity_of_tweet = TextBlob(cleaned_sentence).sentiment.polarity\n","  # print(polarity_of_tweet)\n","\n","\n","\n","\n","\n","  # count specific words (speakers' first or family names)\n","  yoshua_name = ['yoshua' , 'bengio', 'yoshuabengio']\n","  gary_name = ['gary' , 'marcus', 'garymarcus']\n","\n","  has_yoshua = any(word in cleaned_sentence for word in yoshua_name)\n","  has_gary = any(word in cleaned_sentence for word in gary_name)\n","\n","\n","  a = pd.Series({'is_english': is_english, 'polarity_of_tweet':polarity_of_tweet, 'has_yoshua':has_yoshua, 'has_gary':has_gary})\n","  return a\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kx17fsdzJu5V","colab_type":"text"},"source":["# Analyze All Tweets"]},{"cell_type":"code","metadata":{"id":"FDQBn65oOb_d","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":237},"outputId":"72e09f57-c52f-4b02-d1ac-82a4dbaba072","executionInfo":{"status":"ok","timestamp":1578017617732,"user_tz":-210,"elapsed":172085,"user":{"displayName":"Peyman Ghasemi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCc9ANnnVEQ6qW-B2mMkrQfM-IdrpaQNO8I-C83=s64","userId":"11152285370379683023"}}},"source":["import json\n","import pandas as pd\n","# import TweetEmotion\n","\n","# csv file of the data\n","file = '/content/gdrive/My Drive/Colab Notebooks/Twitter_Analysis/ALL_tweets_with_hashtag_AIDebate.csv'\n","\n","# read as pandas dataframe\n","df = pd.read_csv(file)\n","\n","# standardize the time\n","df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S')\n","\n","# initialize emotion prediction model\n","tweet_emotion = TweetEmotion()\n","\n","# determine emotion\n","# df['emotions'] = df['text'].apply(tweet_emotion.probability_of_emotion)\n","df[['Anger', 'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise']] = df['text'].apply(tweet_emotion.probability_of_emotion)\n","\n","# determine language of tweet, analyze its sentiment, and frequency of speakers' names\n","df[['is_english',  'polarity_of_tweet',  'has_yoshua',  'has_gary']] = df['text'].apply(sentiment_and_freq)\n","                     \n","\n","print(df['date'])\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["0     2019-12-31 17:27:05\n","1     2019-12-31 17:11:53\n","2     2019-12-31 17:04:32\n","3     2019-12-31 14:36:41\n","4     2019-12-31 14:00:51\n","              ...        \n","435   2019-12-22 18:59:43\n","436   2019-12-22 18:30:11\n","437   2019-12-22 15:06:56\n","438   2019-12-22 15:01:19\n","439   2019-12-22 14:59:50\n","Name: date, Length: 440, dtype: datetime64[ns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"P0Vq3IYEKNhV","colab_type":"text"},"source":["# Save the Results"]},{"cell_type":"code","metadata":{"id":"hbZGtZZVJsWy","colab_type":"code","colab":{}},"source":["save_path = '/content/gdrive/My Drive/Colab Notebooks/Twitter_Analysis/Analyzed_tweets.csv'\n","df.to_csv(save_path)"],"execution_count":0,"outputs":[]}]}