{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LoadDataAndAnalyze.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9elFTeRBJ2-n","colab_type":"text"},"source":["# Initial Requirements"]},{"cell_type":"code","metadata":{"id":"k4RQTwreMWpU","colab_type":"code","outputId":"56b2233c-3900-46ab-8dc1-7265a1c9b8b2","executionInfo":{"status":"ok","timestamp":1578150517642,"user_tz":-210,"elapsed":188434,"user":{"displayName":"Peyman Ghasemi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCc9ANnnVEQ6qW-B2mMkrQfM-IdrpaQNO8I-C83=s64","userId":"11152285370379683023"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Mount your google drive to the colab environment\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# copy data\n","# !cp '/content/gdrive/My Drive/Colab Notebooks/Twitter_Analysis/OLD_tweets_with_hashtag_AIDebate.txt' before_debate.txt\n","# !cp '/content/gdrive/My Drive/Colab Notebooks/Twitter_Analysis/tweets_with_hashtag_AIDebate1.txt' after_debate.txt\n","\n","\n","\n","\n","# change enviroment variable of keras's backend to theano\n","import os; os.environ['KERAS_BACKEND'] = 'theano'\n","\n","# get and install emotion predictor model\n","!git clone https://github.com/nikicc/twitter-emotion-recognition.git \n","!ls # list of files\n","\n","# go to the directory\n","%cd twitter-emotion-recognition/\n","\n","# install required versions of the libraries\n","!pip install -r requirements.txt\n","\n","\n","\n","\n","\n","# install library for language detection\n","!pip install langdetect\n","\n","# download NLTK packages\n","import nltk\n","!python -m nltk.downloader all"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n","Cloning into 'twitter-emotion-recognition'...\n","remote: Enumerating objects: 54, done.\u001b[K\n","remote: Total 54 (delta 0), reused 0 (delta 0), pack-reused 54\u001b[K\n","Unpacking objects: 100% (54/54), done.\n","gdrive\tsample_data  twitter-emotion-recognition\n","/content/twitter-emotion-recognition\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.25.3)\n","Collecting keras==1.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/5e/7e64f15f0e5ae65a29c738fc261ce1e0a72d92acfc45f06ef906c6e84bf2/Keras-1.1.0.tar.gz (150kB)\n","\u001b[K     |████████████████████████████████| 153kB 2.7MB/s \n","\u001b[?25hCollecting theano==0.8.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/3d/2354fac96ca9594b755ec22d91133522a7db0caa0877165a522337d0ed73/Theano-0.8.2.tar.gz (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 46.5MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 1)) (2.6.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 1)) (1.17.4)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 1)) (2018.9)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==1.1.0->-r requirements.txt (line 2)) (3.13)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras==1.1.0->-r requirements.txt (line 2)) (1.12.0)\n","Requirement already satisfied: scipy>=0.11 in /usr/local/lib/python3.6/dist-packages (from theano==0.8.2->-r requirements.txt (line 3)) (1.3.3)\n","Building wheels for collected packages: keras, theano\n","  Building wheel for keras (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras: filename=Keras-1.1.0-cp36-none-any.whl size=178686 sha256=c77b0f56ba90f6e0330d5624d33bb5ecba892b5c6d13c5d4fb8859b45c7c6275\n","  Stored in directory: /root/.cache/pip/wheels/ae/83/3e/c42ce0672e537640ee706143ebdd1dd691b7693b4ca50f72a8\n","  Building wheel for theano (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for theano: filename=Theano-0.8.2-cp36-none-any.whl size=2725411 sha256=8a52a8ad9a5694aa0e1a40db3b32fc9ee0a9656f167dfcccfa2edc172ec65a21\n","  Stored in directory: /root/.cache/pip/wheels/e4/33/9d/880c86cc0db0b29d34fb6c62256322a4ea1694404192c2b408\n","Successfully built keras theano\n","\u001b[31mERROR: textgenrnn 1.4.1 has requirement keras>=2.1.5, but you'll have keras 1.1.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: pymc3 3.7 has requirement theano>=1.0.4, but you'll have theano 0.8.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: kapre 0.1.3.1 has requirement keras>=2.0.0, but you'll have keras 1.1.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: fancyimpute 0.4.3 has requirement keras>=2.0.0, but you'll have keras 1.1.0 which is incompatible.\u001b[0m\n","Installing collected packages: theano, keras\n","  Found existing installation: Theano 1.0.4\n","    Uninstalling Theano-1.0.4:\n","      Successfully uninstalled Theano-1.0.4\n","  Found existing installation: Keras 2.2.5\n","    Uninstalling Keras-2.2.5:\n","      Successfully uninstalled Keras-2.2.5\n","Successfully installed keras-1.1.0 theano-0.8.2\n","Collecting langdetect\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/59/4bc44158a767a6d66de18c4136c8aa90491d56cc951c10b74dd1e13213c9/langdetect-1.0.7.zip (998kB)\n","\u001b[K     |████████████████████████████████| 1.0MB 2.7MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect) (1.12.0)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.7-cp36-none-any.whl size=993460 sha256=cea115b26cbfc804dabd833988a626b637a667796b16749bb836abfcd9d13f3a\n","  Stored in directory: /root/.cache/pip/wheels/ec/0c/a9/1647275e7ef5014e7b83ff30105180e332867d65e7617ddafe\n","Successfully built langdetect\n","Installing collected packages: langdetect\n","Successfully installed langdetect-1.0.7\n","/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n","  warn(RuntimeWarning(msg))\n","[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/abc.zip.\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/alpino.zip.\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown.zip.\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/chat80.zip.\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/city_database.zip.\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2000.zip.\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2002.zip.\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/crubadan.zip.\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dolch.zip.\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/floresta.zip.\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ieer.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/indian.zip.\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/kimmo.zip.\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/moses_sample.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/omw.zip.\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/paradigms.zip.\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pil.zip.\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pl196x.zip.\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ppattach.zip.\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ptb.zip.\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/qc.zip.\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/rte.zip.\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/senseval.zip.\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/smultron.zip.\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/state_union.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/swadesh.zip.\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/switchboard.zip.\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/timit.zip.\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/toolbox.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr.zip.\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr2.zip.\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet.zip.\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/webtext.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet.zip.\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ycoe.zip.\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/rslp.zip.\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Unzipping help/tagsets.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping\n","[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/perluniprops.zip.\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Jo_cDZA-MoBa","colab_type":"text"},"source":["# Emotion Analysis Part"]},{"cell_type":"code","metadata":{"id":"vRzOMNpvMlcw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"9ff81983-70a4-4edd-a27e-a30d61afb1d0","executionInfo":{"status":"ok","timestamp":1578150526862,"user_tz":-210,"elapsed":197643,"user":{"displayName":"Peyman Ghasemi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCc9ANnnVEQ6qW-B2mMkrQfM-IdrpaQNO8I-C83=s64","userId":"11152285370379683023"}}},"source":["\n","\n","\n","# analyze each tweet\n","import datetime\n","from emotion_predictor import EmotionPredictor\n","\n","\n","\n","class TweetEmotion:\n","  \n","  # load emotion prediction model\n","  def __init__(self):\n","    self.model = EmotionPredictor(classification='ekman', setting='mc')\n","\n","\n","  # what emotion?\n","  def what_emotion(self, tweet):\n","    prediction = self.model.predict_classes([tweet])\n","    # print('The associated emotion with [' + tweet + '] seems to be:' + '\\n' + str(prediction))\n","    return prediction\n","\n","  # probability of emotion\n","  def probability_of_emotion(self, tweet):\n","    probability = self.model.predict_probabilities([tweet])\n","    # print('The associated probablity of emotion with [' + tweet + '] seems to be:' + '\\n' + str(probability))\n","    # return [probability['Anger'], probability['Disgust'], probability['Fear'],probability['Joy'], probability['Sadness'], probability['Surprise']]\n","    p = pd.Series({'Anger': float(probability['Anger']),'Disgust': float(probability['Disgust']), 'Fear': float(probability['Fear']), 'Joy': float(probability['Joy']), 'Sadness': float(probability['Sadness']),  'Surprise': float(probability['Surprise'])})\n","    return p\n","\n","\n","\n","# tweet_emotion = TweetEmotion()\n","\n","\n","\n","\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using Theano backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"72CC1L1PYk2I","colab_type":"text"},"source":["# Tweet Cleaning and Sentiment Analysis"]},{"cell_type":"code","metadata":{"id":"qvjxUX0iYsxM","colab_type":"code","colab":{}},"source":["from textblob import TextBlob\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize.treebank import TreebankWordDetokenizer\n","import langdetect\n","\n","def sentiment_and_freq(tweet):\n","\n","  # check whether tweet's language is English\n","  is_english = False\n","  lang = langdetect.detect(tweet)\n","  if(lang != 'en'):\n","    is_english = False\n","    # print(\"non-English\")\n","  else:\n","    # print(\"Yeah, English\")\n","    is_english = True\n","\n","\n","  # Clean the Tweet\n","\n","  # split into words\n","  tokens = word_tokenize(tweet)\n","  # convert to lower case\n","  tokens = [w.lower() for w in tokens]\n","  # remove punctuation from each word\n","  import string\n","  table = str.maketrans('', '', string.punctuation)\n","  stripped = [w.translate(table) for w in tokens]\n","  # remove remaining tokens that are not alphabetic\n","  words = [word for word in stripped if word.isalpha()]\n","  # filter out stop words\n","  from nltk.corpus import stopwords\n","  stop_words = set(stopwords.words('english'))\n","  words = [w for w in words if not w in stop_words]\n","\n","\n","\n","\n","  # detokenize to calculate sentiment polarity\n","  cleaned_sentence = TreebankWordDetokenizer().detokenize(words)\n","  polarity_of_tweet = TextBlob(cleaned_sentence).sentiment.polarity\n","  # print(polarity_of_tweet)\n","\n","\n","\n","\n","\n","  # count specific words (speakers' first or family names)\n","  yoshua_name = ['yoshua' , 'bengio', 'yoshuabengio']\n","  gary_name = ['gary' , 'marcus', 'garymarcus']\n","\n","  has_yoshua = any(word in cleaned_sentence for word in yoshua_name)\n","  has_gary = any(word in cleaned_sentence for word in gary_name)\n","\n","\n","    # count specific words (deep learning, symbol, hybrid)\n","  deeplearning = ['deeplearning' , 'deep learning', 'dl', 'deep_learning']\n","  symbol = ['symbol' , 'symbols', 'symbolic']\n","  hybrid = ['hybrid']\n","\n","  has_deeplearning = any(word in cleaned_sentence for word in deeplearning)\n","  has_symbol = any(word in cleaned_sentence for word in symbol)\n","  has_hybrid = any(word in cleaned_sentence for word in hybrid)\n","\n","\n","  a = pd.Series({'is_english': is_english, 'polarity_of_tweet':polarity_of_tweet, 'has_yoshua':has_yoshua, 'has_gary':has_gary, \n","                 'has_deeplearning':has_deeplearning, 'has_symbol':has_symbol, 'has_hybrid':has_hybrid})\n","  return a\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kx17fsdzJu5V","colab_type":"text"},"source":["# Analyze All Tweets"]},{"cell_type":"code","metadata":{"id":"FDQBn65oOb_d","colab_type":"code","outputId":"2aee4a9d-e47f-469c-e14e-ee0f0fab25c9","executionInfo":{"status":"ok","timestamp":1578150817352,"user_tz":-210,"elapsed":488118,"user":{"displayName":"Peyman Ghasemi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCc9ANnnVEQ6qW-B2mMkrQfM-IdrpaQNO8I-C83=s64","userId":"11152285370379683023"}},"colab":{"base_uri":"https://localhost:8080/","height":258}},"source":["import json\n","import pandas as pd\n","# import TweetEmotion\n","\n","# csv file of the data\n","file = '/content/gdrive/My Drive/Colab Notebooks/Twitter_Analysis/ALL_tweets_with_hashtag_AIDebate.csv'\n","\n","# read as pandas dataframe\n","df = pd.read_csv(file)\n","\n","# standardize the time\n","df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S')\n","\n","# initialize emotion prediction model\n","tweet_emotion = TweetEmotion()\n","\n","# determine emotion\n","df[['Anger', 'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise']] = df['text'].apply(tweet_emotion.probability_of_emotion)\n","\n","# determine language of tweet, analyze its sentiment, and frequency of speakers' names\n","df[['is_english',  'polarity_of_tweet',  'has_yoshua',  'has_gary', 'has_deeplearning', 'has_symbol', 'has_hybrid']] = df['text'].apply(sentiment_and_freq)\n","                     \n","\n","print(df['date'])\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["WARNING (theano.tensor.blas): We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"],"name":"stderr"},{"output_type":"stream","text":["0     2019-12-31 17:27:05\n","1     2019-12-31 17:11:53\n","2     2019-12-31 17:04:32\n","3     2019-12-31 14:36:41\n","4     2019-12-31 14:00:51\n","              ...        \n","451   2019-12-17 16:10:34\n","452   2019-12-17 02:11:19\n","453   2019-12-17 00:14:05\n","454   2019-12-16 13:46:20\n","455   2019-12-16 07:31:11\n","Name: date, Length: 456, dtype: datetime64[ns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"P0Vq3IYEKNhV","colab_type":"text"},"source":["# Save the Results"]},{"cell_type":"code","metadata":{"id":"hbZGtZZVJsWy","colab_type":"code","colab":{}},"source":["save_path = '/content/gdrive/My Drive/Colab Notebooks/Twitter_Analysis/Analyzed_tweets.csv'\n","df.to_csv(save_path)"],"execution_count":0,"outputs":[]}]}